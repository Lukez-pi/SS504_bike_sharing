---
title: "Project"
output: html_document
---
## Loading data and cleaning
```{r}
library(faraway)
library(leaps)
library(corrplot)
library(MASS)
library(ggplot2)
library(rpart)
library(rattle) #ET: d/l this alter
library(rpart.plot) #and this
library(RColorBrewer)
library(tidyr)
library(stringr)
library(chron)
library(dplyr)

```

```{r}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))  
dir=(dirname(rstudioapi::getActiveDocumentContext()$path))
source("dataCleaning.R")

```

```{r}
wkd_bike <- filter(bike, is_weekend == 1)
hol_bike <- filter(bike, is_holiday == 1)
work_bike <- filter(bike, work == 1)
```

```{r}
# Plot counts per hour on weekend, holiday, workday
par(mfrow = c(3, 1), mar=c(4, 4, 2, 1))
plot(hol_bike$hour, hol_bike$cnt, main = "Holiday", xlab = "Hour", ylab = "Count", ylim = c(0, 7600))
plot(wkd_bike$hour, wkd_bike$cnt, main = "Weekend", xlab = "Hour", ylab = "Count", ylim = c(0, 7600))
plot(work_bike$hour, work_bike$cnt, main = "Workday", xlab = "Hour", ylab = "Count", ylim = c(0, 7600))
```

```{r}
#specify models to try
formula1 <- cnt ~ t1 + t2 + hum + wind_speed + weather_code + is_holiday + is_weekend + season + hour
formula2 <- cnt ~ t2 + hum + wind_speed + weather_code + hour + weekday
formula3 <- cnt ~ t2 + hum + wind_speed + weather_code + hour
```

```{r}
data = bike
rm(bike)
```

Running decision trees on 1. all 2. weekend 3. holiday 4. workday data
```{r}
# All days
all <- rpart(cnt ~ hour, data = data)
png("decision_hour.png")
fancyRpartPlot(all)
#dev.off()

# Weekend
wkd <- rpart(cnt ~ hour, data = wkd_bike)
fancyRpartPlot(wkd)

# Holiday
hol <- rpart(cnt ~ hour, data = hol_bike)
#fancyRpartPlot(hol)

# Work
work <- rpart(cnt ~ hour, data = work_bike)
#fancyRpartPlot(work)

#wkd/hol
wkd_hol <- rpart(cnt ~ hour, data = filter(data, work == 0))
```
Here are the hour groupings depending on type of day. I am unsure if there is one grouping that works for both workdays and weekend/holiday.

All days:
G1 (Sleeping): 22, 23, 0 - 6
G2: 10, 11, 20, 21
G3: 7, 9 , 12 - 16, 19
G4 (Peak): 8, 17, 18

Weekend (same if you group weekend and holiday)
G1: 1 - 7
G2: 0, 8, 9, 20 - 23
G3: 10, 11, 18, 19
G4: 12 - 17

Holiday
G1: 0-8, 21 - 23
G2: 9, 10, 19, 20
G3: 11, 17, 18
G4: 12 - 16

Workday:
G1: 0 - 5
G2: 6, 22, 23
G3: 10 - 15, 20, 21
G4: 7, 9, 16, 19
G5: 17, 18
G6: 8

Our groups by eye:
Peak rush: 8, 17, 18
off peak: 7, 9, 16, 19
midday: 10:15
Night: 20:23
sleeping: 0-6

```{r}
# Here I'm making hour_group variables for each day type. We shouldn't run the model on the full data since there's redundant information in these columns

hour_data <- data %>% mutate(
  all_hrs = ifelse(hour %in% c(7, 9 , 12:16, 19), "off_peak", 
                   ifelse(hour %in% c(22, 23, 0:6), "sleeping",
                          ifelse(hour %in% c(10, 11, 20, 21), "morn_eve",
                                 "peak"))),
  wkdhol_hrs = ifelse(hour %in% c(1:7), "wkdhol_sleeping",
                   ifelse(hour %in% c(0, 8, 9, 20:23), "wkdhol_8-9am_and_10pm-12am", 
                          ifelse(hour %in% c(10, 11, 18, 19), "wkdhol_10-11am_and_6-7pm",
                                 "wkdhol_12pm-5pm"))),
  hol_hrs = ifelse(hour %in% c(0:8, 21:23), "hol_sleeping", 
                   ifelse(hour %in% c(9, 10, 19, 20), "hol_8-9am_8-9pm",
                          ifelse(hour %in% c(11, 17, 18), "hol_11am_and_5-6pm",
                                 "hol_midday"))),
  work_hrs = ifelse(hour %in% c(0:5), "workday_sleeping",
                    ifelse(hour %in% c(6, 22, 23), "workday_6am_and_10-11pm",
                           ifelse(hour %in% c(10:15, 20, 21), "workday_10am-3pm_and_10-11pm",
                                  ifelse(hour %in% c(7, 9, 16, 19), "workday_7-9am_and_4-5pm",
                                         ifelse(hour %in% c(17, 18), "workday_5-6pm_rushhour",
                                                "workday_8am_rushhour"))))),
  eye_hrs = ifelse(hour %in% c(0:6), "sleeping",
                   ifelse(hour %in% c(20:23), "night",
                          ifelse(hour %in% c(10:15), "midday",
                                 ifelse(hour %in% c(7, 9, 16, 19), "off_peak", 
                                        "peak")))),
#ET: combine work hrs & wkdhol into one group :
    all_hrs2= ifelse(work%in% c(1), work_hrs, wkdhol_hrs)
    )

  
  
```

Testing lms based on different hour groups

```{r}
# Hour grouping with all days
# Adjusted R-squared: 0.69
summary(lm(cnt ~ t2 + hum + wind_speed + weather_code + season + work + all_hrs, data = hour_data))

# Hour grouping with all days + interaction of work/all_hrs
# Adjusted R-squared: 0.79
summary(lm(cnt ~ t2 + hum + wind_speed + weather_code + season + work + all_hrs + work:all_hrs, data = hour_data))

# All 24 hours
# Adjusted R-squared: 0.71
summary(lm(cnt ~ t2 + hum + wind_speed + weather_code + season + work + hour, data = hour_data))

# All 24 hours + interaction of work/all 24 hours
# Adjusted R-squared: 0.90
#ET this also adds a lot more vars, maybe we should do a bic test too
summary(lm(cnt ~ t2 + hum + wind_speed + weather_code + season + work + hour + work:hour, data = hour_data))

# Hour grouping by eye (most interpretable)
# Adjusted R-squared: 0.83 (why is this bigger than for all_hrs above?)
#ET: a little surprising, there is one more var in eye_hours comapred to all_hrs? 
summary(lm(cnt ~ t2 + hum + wind_speed + weather_code + season + work + eye_hrs + work:eye_hrs, data = hour_data))

#ET: hour grouping using combo all_hours2 var (this requires less var and does about teh same IN terms of adj-R2 as fully work:hour)
# Adjusted R-squared: 0.89 
summary(lm(cnt ~ t2 + hum + wind_speed + weather_code + season + all_hrs2, data = hour_data))
```

Some plots
```{r}
plot(temp$hour, temp$`mean(cnt)`, main = "Average rentals by hour", xlab = "Hour", ylab = "Avg Rentals")
```

```{r}
# Scatter plots
#png("lognumerics.png")
data %>% mutate(log_cnt = log(cnt)) %>% select(-c(1, 6:ncol(data))) %>%
  pivot_longer(-log_cnt, values_to = "NumericVariables") %>% 
  ggplot(aes(x = NumericVariables, y = log_cnt)) +
    facet_wrap(~ name, scales = "free") +
    geom_point(alpha = 0.1)
#dev.off()
# Numeric variables
#pairs(data[,1:5])[1]
```

## Colinearity
```{r}
lm1 = lm(formula1, data = data)
par(mfrow=c(2,2))
plot(lm1, which=c(1,2,3,4))
vif(lm1)
print("R^2")
c(summary(lm1)$r.squared)
BIC(lm1)

#png("num_corr.png")
corrplot(cor(select(data, t1, t2, hum, wind_speed)))
#dev.off()
```
This suggests us to remove t1. 


## Vanilla Linear Regression
```{r}
reduced_data = subset(data, select=-c(t1, day, month, year, weekday, work))
lm_normal = lm(cnt~. ,data=reduced_data)
par(mfrow=c(2,2))
plot(lm_normal, which=c(1,2,3,4))
BIC(lm_normal)
```

## 
We can also try to use BoxCox to transform our data.
```{r}
boxFit = lm(cnt~. ,data=reduced_data[-2016,])
boxcox(boxFit, plotit=T)
abline(v=0.14)
```
The best lambda seem to be roughly 0.14 Therefore, the transforemed response $g_{\lambda}(y) = \frac{y^{(0.14)}-1}{0.14} = 2-2y^{-0.5}$
```{r}
transformed = reduced_data[-2016,]
log_transformed = reduced_data[-2016,]
log_transformed$cnt = log(log_transformed$cnt) 
transformed$cnt = (transformed$cnt^0.14-1)/0.14
lm_transformed_normal = lm(cnt~. ,data=transformed)
lm_log_normal = lm(cnt~. ,data=log_transformed)
par(mfrow=c(2,2))
plot(lm_transformed_normal, which=c(1,2,3,4))
```
```{r}
plot(lm_log_normal, which=c(1,2,3,4))
BIC(lm_transformed_normal)
BIC(lm_log_normal)
```


The upper tail fits much better after transformation. But the lower tail seemed to be worse. May consider use Logistic regression to separate our data in to two categories.

We can also review the residuals in the model of log_transformation.
```{r}
original_y = exp(lm_log_normal$fitted.values+(summary(lm_log_normal)$sigma)^2/2)
qqnorm(original_y-reduced_data$cnt)
```


```{r}
fit0 = glm(cnt~., data=reduced_data, family ="gaussian")
fit_poi = glm(cnt~., data=reduced_data, family ="poisson")
fit_gamma_log = glm(cnt~., data=reduced_data[-2016,] , family =Gamma(link="log"))
#fit_gamma_identity = glm(cnt~., data=reduced_data[-2016,], family =Gamma(link="identity"))
BIC(fit0)
BIC(fit_poi)
BIC(fit_gamma_log)
```
It seemed that a fit with Gamma distribution is also good.

To look for best model under the gamma fit and linear fit, may choose forward or backward selection. Subset is almost impossible as we have too many dummy variables, boosting the number of possible models.

### LOOCV for different models
```{r}
loocv.lm1 <- function(md1) {
  return(mean((residuals(md1)/(1-hatvalues(md1)))^2))
}
loocv.lm2 <- function(res, hat) {
  return(mean((res/(1-hat))^2))
}

loocv.lm1(fit_poi)
loocv.lm1(fit_gamma_log)
loocv.lm1(lm_normal)
loocv.lm2(original_y-reduced_data$cnt, exp(lm_log_normal$fitted.values+(summary(lm_log_normal)$sigma)^2/2))

```

# LOOCV - Gamma
```{r}
non_neg_data = reduced_data[-2016,]
res = 0
i = 1
while (i < nrow(non_neg_data)) {
  curr_fit = glm(cnt~., data=non_neg_data[-c(i:min(i+499, nrow(non_neg_data))),], family=Gamma(link="log")) # Fit model by removing the i-th row.
  new = non_neg_data[c(i:min(i+499, nrow(non_neg_data))),]
  res = res + sum((predict(curr_fit, new[-1], se.fit = TRUE)$fit - new$cnt)^2) # Predict the value, compute the squared residual.
  i = i + 500
}
print(res/nrow(non_neg_data))
```

# LOOCV-Poisson
```{r}
res = 0
i = 1
while (i < nrow(reduced_data)) {
  print(i)
  curr_fit = glm(cnt~., data=reduced_data[-c(i:min(i+499, nrow(reduced_data))),], family="poisson") # Fit model by removing the i-th row.
  new = reduced_data[c(i:min(i+499, nrow(reduced_data))),]
  res = res + sum((predict(curr_fit, new[-1], se.fit = TRUE)$fit - new$cnt)^2) # Predict the value, compute the squared residual.
  i = i + 500
}
print(res/nrow(reduced_data))
```
```{r}
res = 0
i = 1
while (i < nrow(reduced_data)) {
  curr_fit = lm(cnt~., data=reduced_data[-c(i:min(i+499, nrow(reduced_data))),]) # Fit model by removing the i-th row.
  new = reduced_data[c(i:min(i+499, nrow(reduced_data))),]
  res = res + sum((predict(curr_fit, new[-1], se.fit = TRUE)$fit - new$cnt)^2) # Predict the value, compute the squared residual.
  i = i + 500
}
print(res/nrow(reduced_data))
```

# LOOCV - LogNormal
```{r}
res = 0
i = 1

temp = reduced_data[-2016,]
while (i < nrow(reduced_data)) {
  curr_fit = lm(cnt~., data=log_transformed[-c(i:min(i+499, nrow(log_transformed))),]) # Fit model by removing the i-th row.
  new = temp[c(i:min(i+499, nrow(temp))),]
  fitted = predict(curr_fit, new[-1], se.fit = TRUE)$fit
  res = res + sum((exp(fitted+(summary(curr_fit)$sigma)^2/2) - new$cnt)^2) # Predict the value, compute the squared residual.
  i = i + 500
}
print(res/nrow(reduced_data))
```

# backward/forward selection 
```{r}
num_data = nrow(log_transformed)
lm_log_normal_empty = lm(cnt~1 ,data=log_transformed)
lm_log_normal_full = lm_log_normal
fit_backward = step(lm_log_normal_full, direction="backward", k=log(num_data))
fit_forward = step(lm_log_normal_empty, scope= list(upper=lm_log_normal_full, lower=lm_log_normal_empty), direction="forward", k = log(num_data))
```