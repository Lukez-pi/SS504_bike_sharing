---
title: "Project"
output: html_document
---
## Loading data and cleaning
```{r}
library(faraway)
library(leaps)
library(corrplot)
library(MASS)
library(ggplot2)
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
```

```{r}
source("~/Desktop/SS504_bike_sharing/dataCleaning.R")
```

```{r}
# Plot counts per hour on weekend, holiday, workday
wkd_bike <- filter(bike, is_weekend == 1)
hol_bike <- filter(bike, is_holiday == 1)
work_bike <- filter(bike, work == 1)
par(mfrow = c(3, 1), mar=c(4, 4, 2, 1))
plot(hol_bike$hour, hol_bike$cnt, main = "Holiday", xlab = "Hour", ylab = "Count", ylim = c(0, 7600))
plot(wkd_bike$hour, wkd_bike$cnt, main = "Weekend", xlab = "Hour", ylab = "Count", ylim = c(0, 7600))
plot(work_bike$hour, work_bike$cnt, main = "Workday", xlab = "Hour", ylab = "Count", ylim = c(0, 7600))
```

```{r}
formula1 <- cnt ~ t1 + t2 + hum + wind_speed + weather_code + is_holiday + is_weekend + season + hour
formula2 <- cnt ~ t2 + hum + wind_speed + weather_code + hour + weekday
formula3 <- cnt ~ t2 + hum + wind_speed + weather_code + hour
```

```{r}
data = bike
```

```{r}
# All days
all <- rpart(cnt ~ hour, data = data)
#png("decision_hour.png")
fancyRpartPlot(all)
#dev.off()

# Weekend
wkd <- rpart(cnt ~ hour, data = wkd_bike)
fancyRpartPlot(wkd)

# Holiday
hol <- rpart(cnt ~ hour, data = hol_bike)
fancyRpartPlot(hol)

# Work
work <- rpart(cnt ~ hour, data = work_bike)
fancyRpartPlot(work)

#wkd/hol
wkd_hol <- rpart(cnt ~ hour, data = filter(bike, work == 0))

```
All days:
G1 (Sleeping): 22, 23, 0 - 6
G2: 10, 11, 20, 21
G3: 7, 9 , 12 - 16, 19
G4 (Peak): 8, 17, 18

Weekend (same if you group weekend and holiday)
G1: 1 - 7
G2: 0, 8, 9, 20 - 23
G3: 10, 11, 18, 19
G4: 12 - 17

Holiday
G1: 0-8, 21 - 23
G2: 9, 10, 19, 20
G3: 11, 17, 18
G4: 12 - 16

Workday:
G1: 0 - 5
G2: 6, 22, 23
G3: 10 - 15, 20, 21
G4: 7, 9, 16, 19
G5: 17, 18
G6: 8

Our idea:
Peak rush: 8, 17 18
off peak: 7, 9, 16, 19
midday: 6, 10 - 15, 20, 21
sleeping: 23, 0-5

```{r}
hour_data <- data %>% mutate(
  all_hrs = ifelse(hour %in% c(7, 9 , 12:16, 19), "offpeak", 
                   ifelse(hour %in% c(22, 23, 0:6), "sleeping",
                          ifelse(hour %in% c(10, 11, 20, 21), "morn_eve",
                                 "peak"))),
  wkdhol_hrs = ifelse(hour %in% c(1:7), "sleeping",
                   ifelse(hour %in% c(0, 8, 9, 20:23), "off_morn_eve", 
                          ifelse(hour %in% c(10, 11, 18, 19), "morn_eve",
                                 "midday"))),
  hol_hrs = ifelse(hour %in% c(0:8, 21:23), "sleeping", 
                   ifelse(hour %in% c(9, 10, 19, 20), "off_morn_eve",
                          ifelse(hour %in% c(11, 17, 18), "morn_eve",
                                 "midday"))),
  work_hrs = ifelse(hour %in% c(0:5), "sleeping",
                    ifelse(hour %in% c(6, 22, 23), "edge",
                           ifelse(hour %in% c(10:15, 20, 21), "mid_eve",
                                  ifelse(hour %in% c(7, 9, 16, 19), "off_peak",
                                         ifelse(hour %in% c(17, 18), "peak1",
                                                "peak2")))))
)
```

Testing lms based on different hour groups
```{r}
summary(lm(cnt ~ t2 + hum + wind_speed + weather_code + season + work + all_hrs, data = hour_data))

summary(lm(cnt ~ t2 + hum + wind_speed + weather_code + season + work + all_hrs + work:all_hrs, data = hour_data))

summary(lm(cnt ~ t2 + hum + wind_speed + weather_code + season + work + hour, data = hour_data))
```

```{r}
# Scatter plots
#png("lognumerics.png")
data %>% mutate(log_cnt = log(cnt)) %>% select(-c(1, 6:ncol(data))) %>%
  pivot_longer(-log_cnt, values_to = "NumericVariables") %>% 
  ggplot(aes(x = NumericVariables, y = log_cnt)) +
    facet_wrap(~ name, scales = "free") +
    geom_point(alpha = 0.1)
#dev.off()
# Numeric variables
#pairs(data[,1:5])[1]
```

## Colinearity
```{r}
lm1 = lm(formula1, data = data)
par(mfrow=c(2,2))
plot(lm1, which=c(1,2,3,4))
vif(lm1)
print("R^2")
c(summary(lm1)$r.squared)
BIC(lm1)
```
This suggests us to remove t1. 


## Vanilla Linear Regression
```{r}
reduced_data = subset(data, select=-c(t1, day, month, year, weekday, work))
lm_normal = lm(cnt~. ,data=reduced_data)
par(mfrow=c(2,2))
plot(lm_normal, which=c(1,2,3,4))
BIC(lm_normal)
```

## 
We can also try to use BoxCox to transform our data.
```{r}
boxFit = lm(cnt~. ,data=reduced_data[-2016,])
boxcox(boxFit, plotit=T)
abline(v=0.14)
```
The best lambda seem to be roughly 0.14 Therefore, the transforemed response $g_{\lambda}(y) = \frac{y^{(0.14)}-1}{0.14} = 2-2y^{-0.5}$
```{r}
transformed = reduced_data[-2016,]
log_transformed = reduced_data[-2016,]
log_transformed$cnt = log(log_transformed$cnt) 
transformed$cnt = (transformed$cnt^0.14-1)/0.14
lm_transformed_normal = lm(cnt~. ,data=transformed)
lm_log_normal = lm(cnt~. ,data=log_transformed)
par(mfrow=c(2,2))
plot(lm_transformed_normal, which=c(1,2,3,4))
```
```{r}
plot(lm_log_normal, which=c(1,2,3,4))
BIC(lm_transformed_normal)
BIC(lm_log_normal)
```


The upper tail fits much better after transformation. But the lower tail seemed to be worse. May consider use Logistic regression to separate our data in to two categories.

We can also review the residuals in the model of log_transformation.
```{r}
original_y = exp(lm_log_normal$fitted.values+(summary(lm_log_normal)$sigma)^2/2)
qqnorm(original_y-reduced_data$cnt)
```


```{r}
fit0 = glm(cnt~., data=reduced_data, family ="gaussian")
fit_poi = glm(cnt~., data=reduced_data, family ="poisson")
fit_gamma_log = glm(cnt~., data=reduced_data[-2016,] , family =Gamma(link="log"))
#fit_gamma_identity = glm(cnt~., data=reduced_data[-2016,], family =Gamma(link="identity"))
BIC(fit0)
BIC(fit_poi)
BIC(fit_gamma_log)
```
It seemed that a fit with Gamma distribution is also good.

To look for best model under the gamma fit and linear fit, may choose forward or backward selection. Subset is almost impossible as we have too many dummy variables, boosting the number of possible models.

### LOOCV for different models
```{r}
loocv.lm1 <- function(md1) {
  return(mean((residuals(md1)/(1-hatvalues(md1)))^2))
}
loocv.lm2 <- function(res, hat) {
  return(mean((res/(1-hat))^2))
}

loocv.lm1(fit_poi)
loocv.lm1(fit_gamma_log)
loocv.lm1(lm_normal)
loocv.lm2(original_y-reduced_data$cnt, exp(lm_log_normal$fitted.values+(summary(lm_log_normal)$sigma)^2/2))

```

# LOOCV - Gamma
```{r}
non_neg_data = reduced_data[-2016,]
res = 0
i = 1
while (i < nrow(non_neg_data)) {
  curr_fit = glm(cnt~., data=non_neg_data[-c(i:min(i+499, nrow(non_neg_data))),], family=Gamma(link="log")) # Fit model by removing the i-th row.
  new = non_neg_data[c(i:min(i+499, nrow(non_neg_data))),]
  res = res + sum((predict(curr_fit, new[-1], se.fit = TRUE)$fit - new$cnt)^2) # Predict the value, compute the squared residual.
  i = i + 500
}
print(res/nrow(non_neg_data))
```

# LOOCV-Poisson
```{r}
res = 0
i = 1
while (i < nrow(reduced_data)) {
  print(i)
  curr_fit = glm(cnt~., data=reduced_data[-c(i:min(i+499, nrow(reduced_data))),], family="poisson") # Fit model by removing the i-th row.
  new = reduced_data[c(i:min(i+499, nrow(reduced_data))),]
  res = res + sum((predict(curr_fit, new[-1], se.fit = TRUE)$fit - new$cnt)^2) # Predict the value, compute the squared residual.
  i = i + 500
}
print(res/nrow(reduced_data))
```
```{r}
res = 0
i = 1
while (i < nrow(reduced_data)) {
  curr_fit = lm(cnt~., data=reduced_data[-c(i:min(i+499, nrow(reduced_data))),]) # Fit model by removing the i-th row.
  new = reduced_data[c(i:min(i+499, nrow(reduced_data))),]
  res = res + sum((predict(curr_fit, new[-1], se.fit = TRUE)$fit - new$cnt)^2) # Predict the value, compute the squared residual.
  i = i + 500
}
print(res/nrow(reduced_data))
```

# LOOCV - LogNormal
```{r}
res = 0
i = 1

temp = reduced_data[-2016,]
while (i < nrow(reduced_data)) {
  curr_fit = lm(cnt~., data=log_transformed[-c(i:min(i+499, nrow(log_transformed))),]) # Fit model by removing the i-th row.
  new = temp[c(i:min(i+499, nrow(temp))),]
  fitted = predict(curr_fit, new[-1], se.fit = TRUE)$fit
  res = res + sum((exp(fitted+(summary(curr_fit)$sigma)^2/2) - new$cnt)^2) # Predict the value, compute the squared residual.
  i = i + 500
}
print(res/nrow(reduced_data))
```

